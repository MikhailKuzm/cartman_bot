[
  {
    "loss": 6.8202,
    "grad_norm": 5.522614479064941,
    "learning_rate": 4.955752212389381e-05,
    "epoch": 0.017699115044247787,
    "step": 10
  },
  {
    "loss": 4.2107,
    "grad_norm": 3.980458974838257,
    "learning_rate": 4.911504424778761e-05,
    "epoch": 0.035398230088495575,
    "step": 20
  },
  {
    "loss": 3.7312,
    "grad_norm": 3.760364294052124,
    "learning_rate": 4.867256637168142e-05,
    "epoch": 0.05309734513274336,
    "step": 30
  },
  {
    "loss": 3.2322,
    "grad_norm": 2.663494348526001,
    "learning_rate": 4.823008849557522e-05,
    "epoch": 0.07079646017699115,
    "step": 40
  },
  {
    "loss": 2.9697,
    "grad_norm": 3.7590363025665283,
    "learning_rate": 4.778761061946903e-05,
    "epoch": 0.08849557522123894,
    "step": 50
  },
  {
    "loss": 2.8234,
    "grad_norm": 2.677386522293091,
    "learning_rate": 4.734513274336283e-05,
    "epoch": 0.10619469026548672,
    "step": 60
  },
  {
    "loss": 2.8258,
    "grad_norm": 1.6777706146240234,
    "learning_rate": 4.690265486725664e-05,
    "epoch": 0.12389380530973451,
    "step": 70
  },
  {
    "loss": 2.7933,
    "grad_norm": 2.0191428661346436,
    "learning_rate": 4.646017699115045e-05,
    "epoch": 0.1415929203539823,
    "step": 80
  },
  {
    "loss": 2.7337,
    "grad_norm": 2.021955966949463,
    "learning_rate": 4.601769911504425e-05,
    "epoch": 0.1592920353982301,
    "step": 90
  },
  {
    "loss": 2.651,
    "grad_norm": 1.8721437454223633,
    "learning_rate": 4.5575221238938055e-05,
    "epoch": 0.17699115044247787,
    "step": 100
  },
  {
    "loss": 2.791,
    "grad_norm": 2.1489315032958984,
    "learning_rate": 4.5132743362831855e-05,
    "epoch": 0.19469026548672566,
    "step": 110
  },
  {
    "loss": 2.6712,
    "grad_norm": 1.876089334487915,
    "learning_rate": 4.469026548672566e-05,
    "epoch": 0.21238938053097345,
    "step": 120
  },
  {
    "loss": 2.6836,
    "grad_norm": 1.9606587886810303,
    "learning_rate": 4.4247787610619477e-05,
    "epoch": 0.23008849557522124,
    "step": 130
  },
  {
    "loss": 2.7619,
    "grad_norm": 1.8509674072265625,
    "learning_rate": 4.380530973451328e-05,
    "epoch": 0.24778761061946902,
    "step": 140
  },
  {
    "loss": 2.5895,
    "grad_norm": 1.795132040977478,
    "learning_rate": 4.3362831858407084e-05,
    "epoch": 0.26548672566371684,
    "step": 150
  },
  {
    "loss": 2.5506,
    "grad_norm": 1.4282540082931519,
    "learning_rate": 4.2920353982300885e-05,
    "epoch": 0.2831858407079646,
    "step": 160
  },
  {
    "loss": 2.4922,
    "grad_norm": 3.2268807888031006,
    "learning_rate": 4.247787610619469e-05,
    "epoch": 0.3008849557522124,
    "step": 170
  },
  {
    "loss": 2.6581,
    "grad_norm": 1.8588542938232422,
    "learning_rate": 4.20353982300885e-05,
    "epoch": 0.3185840707964602,
    "step": 180
  },
  {
    "loss": 2.6349,
    "grad_norm": 1.948283076286316,
    "learning_rate": 4.15929203539823e-05,
    "epoch": 0.336283185840708,
    "step": 190
  },
  {
    "loss": 2.4421,
    "grad_norm": 1.9823461771011353,
    "learning_rate": 4.115044247787611e-05,
    "epoch": 0.35398230088495575,
    "step": 200
  },
  {
    "loss": 2.3965,
    "grad_norm": 1.6776577234268188,
    "learning_rate": 4.0707964601769914e-05,
    "epoch": 0.37168141592920356,
    "step": 210
  },
  {
    "loss": 2.6832,
    "grad_norm": 1.8393977880477905,
    "learning_rate": 4.026548672566372e-05,
    "epoch": 0.3893805309734513,
    "step": 220
  },
  {
    "loss": 2.7041,
    "grad_norm": 1.9676624536514282,
    "learning_rate": 3.982300884955752e-05,
    "epoch": 0.40707964601769914,
    "step": 230
  },
  {
    "loss": 2.5824,
    "grad_norm": 4.928412437438965,
    "learning_rate": 3.938053097345133e-05,
    "epoch": 0.4247787610619469,
    "step": 240
  },
  {
    "loss": 2.6747,
    "grad_norm": 2.0177834033966064,
    "learning_rate": 3.893805309734514e-05,
    "epoch": 0.4424778761061947,
    "step": 250
  },
  {
    "loss": 2.6481,
    "grad_norm": 1.9349029064178467,
    "learning_rate": 3.849557522123894e-05,
    "epoch": 0.46017699115044247,
    "step": 260
  },
  {
    "loss": 2.6733,
    "grad_norm": 2.0935637950897217,
    "learning_rate": 3.8053097345132744e-05,
    "epoch": 0.4778761061946903,
    "step": 270
  },
  {
    "loss": 2.5278,
    "grad_norm": 1.6675294637680054,
    "learning_rate": 3.7610619469026545e-05,
    "epoch": 0.49557522123893805,
    "step": 280
  },
  {
    "loss": 2.6223,
    "grad_norm": 3.3993301391601562,
    "learning_rate": 3.716814159292036e-05,
    "epoch": 0.5132743362831859,
    "step": 290
  },
  {
    "loss": 2.5371,
    "grad_norm": 2.079181432723999,
    "learning_rate": 3.672566371681416e-05,
    "epoch": 0.5309734513274337,
    "step": 300
  },
  {
    "loss": 2.5879,
    "grad_norm": 1.4187179803848267,
    "learning_rate": 3.628318584070797e-05,
    "epoch": 0.5486725663716814,
    "step": 310
  },
  {
    "loss": 2.6141,
    "grad_norm": 1.8780330419540405,
    "learning_rate": 3.5840707964601774e-05,
    "epoch": 0.5663716814159292,
    "step": 320
  },
  {
    "loss": 2.6702,
    "grad_norm": 1.778157114982605,
    "learning_rate": 3.5398230088495574e-05,
    "epoch": 0.584070796460177,
    "step": 330
  },
  {
    "loss": 2.6051,
    "grad_norm": 1.7644641399383545,
    "learning_rate": 3.495575221238938e-05,
    "epoch": 0.6017699115044248,
    "step": 340
  },
  {
    "loss": 2.5262,
    "grad_norm": 1.693527102470398,
    "learning_rate": 3.451327433628319e-05,
    "epoch": 0.6194690265486725,
    "step": 350
  },
  {
    "loss": 2.4197,
    "grad_norm": 1.7237544059753418,
    "learning_rate": 3.407079646017699e-05,
    "epoch": 0.6371681415929203,
    "step": 360
  },
  {
    "loss": 2.5374,
    "grad_norm": 1.8254616260528564,
    "learning_rate": 3.3628318584070804e-05,
    "epoch": 0.6548672566371682,
    "step": 370
  },
  {
    "loss": 2.539,
    "grad_norm": 1.869087815284729,
    "learning_rate": 3.3185840707964604e-05,
    "epoch": 0.672566371681416,
    "step": 380
  },
  {
    "loss": 2.6512,
    "grad_norm": 1.6958625316619873,
    "learning_rate": 3.274336283185841e-05,
    "epoch": 0.6902654867256637,
    "step": 390
  },
  {
    "loss": 2.5366,
    "grad_norm": 1.7554078102111816,
    "learning_rate": 3.230088495575221e-05,
    "epoch": 0.7079646017699115,
    "step": 400
  },
  {
    "loss": 2.5568,
    "grad_norm": 1.7865501642227173,
    "learning_rate": 3.185840707964602e-05,
    "epoch": 0.7256637168141593,
    "step": 410
  },
  {
    "loss": 2.4718,
    "grad_norm": 1.7399524450302124,
    "learning_rate": 3.1415929203539826e-05,
    "epoch": 0.7433628318584071,
    "step": 420
  },
  {
    "loss": 2.4345,
    "grad_norm": 1.7547372579574585,
    "learning_rate": 3.097345132743363e-05,
    "epoch": 0.7610619469026548,
    "step": 430
  },
  {
    "loss": 2.5639,
    "grad_norm": 1.7921918630599976,
    "learning_rate": 3.0530973451327434e-05,
    "epoch": 0.7787610619469026,
    "step": 440
  },
  {
    "loss": 2.5731,
    "grad_norm": 1.8196861743927002,
    "learning_rate": 3.008849557522124e-05,
    "epoch": 0.7964601769911505,
    "step": 450
  },
  {
    "loss": 2.5482,
    "grad_norm": 2.045196294784546,
    "learning_rate": 2.964601769911505e-05,
    "epoch": 0.8141592920353983,
    "step": 460
  },
  {
    "loss": 2.3796,
    "grad_norm": 1.765751838684082,
    "learning_rate": 2.9203539823008852e-05,
    "epoch": 0.831858407079646,
    "step": 470
  },
  {
    "loss": 2.2707,
    "grad_norm": 1.5325063467025757,
    "learning_rate": 2.8761061946902656e-05,
    "epoch": 0.8495575221238938,
    "step": 480
  },
  {
    "loss": 2.6406,
    "grad_norm": 2.183744430541992,
    "learning_rate": 2.831858407079646e-05,
    "epoch": 0.8672566371681416,
    "step": 490
  },
  {
    "loss": 2.4977,
    "grad_norm": 1.751841425895691,
    "learning_rate": 2.7876106194690264e-05,
    "epoch": 0.8849557522123894,
    "step": 500
  },
  {
    "loss": 2.4182,
    "grad_norm": 1.5160435438156128,
    "learning_rate": 2.743362831858407e-05,
    "epoch": 0.9026548672566371,
    "step": 510
  },
  {
    "loss": 2.4467,
    "grad_norm": 2.0434958934783936,
    "learning_rate": 2.6991150442477875e-05,
    "epoch": 0.9203539823008849,
    "step": 520
  },
  {
    "loss": 2.422,
    "grad_norm": 1.9603071212768555,
    "learning_rate": 2.6548672566371686e-05,
    "epoch": 0.9380530973451328,
    "step": 530
  },
  {
    "loss": 2.5448,
    "grad_norm": 1.8362383842468262,
    "learning_rate": 2.610619469026549e-05,
    "epoch": 0.9557522123893806,
    "step": 540
  },
  {
    "loss": 2.4436,
    "grad_norm": 1.7495299577713013,
    "learning_rate": 2.5663716814159294e-05,
    "epoch": 0.9734513274336283,
    "step": 550
  },
  {
    "loss": 2.4217,
    "grad_norm": 1.7866160869598389,
    "learning_rate": 2.5221238938053098e-05,
    "epoch": 0.9911504424778761,
    "step": 560
  },
  {
    "eval_loss": 2.3853490352630615,
    "eval_runtime": 36.5085,
    "eval_samples_per_second": 13.011,
    "eval_steps_per_second": 0.822,
    "epoch": 1.0,
    "step": 565
  },
  {
    "loss": 2.3907,
    "grad_norm": 1.814683437347412,
    "learning_rate": 2.4778761061946905e-05,
    "epoch": 1.008849557522124,
    "step": 570
  },
  {
    "loss": 2.3809,
    "grad_norm": 1.7570185661315918,
    "learning_rate": 2.433628318584071e-05,
    "epoch": 1.0265486725663717,
    "step": 580
  },
  {
    "loss": 2.3416,
    "grad_norm": 1.810420274734497,
    "learning_rate": 2.3893805309734516e-05,
    "epoch": 1.0442477876106195,
    "step": 590
  },
  {
    "loss": 2.3659,
    "grad_norm": 1.9922142028808594,
    "learning_rate": 2.345132743362832e-05,
    "epoch": 1.0619469026548674,
    "step": 600
  },
  {
    "loss": 2.3505,
    "grad_norm": 2.1414437294006348,
    "learning_rate": 2.3008849557522124e-05,
    "epoch": 1.079646017699115,
    "step": 610
  },
  {
    "loss": 2.4023,
    "grad_norm": 1.7869192361831665,
    "learning_rate": 2.2566371681415928e-05,
    "epoch": 1.0973451327433628,
    "step": 620
  },
  {
    "loss": 2.141,
    "grad_norm": 1.4634654521942139,
    "learning_rate": 2.2123893805309738e-05,
    "epoch": 1.1150442477876106,
    "step": 630
  },
  {
    "loss": 2.3245,
    "grad_norm": 2.0014901161193848,
    "learning_rate": 2.1681415929203542e-05,
    "epoch": 1.1327433628318584,
    "step": 640
  },
  {
    "loss": 2.3346,
    "grad_norm": 1.896416425704956,
    "learning_rate": 2.1238938053097346e-05,
    "epoch": 1.1504424778761062,
    "step": 650
  },
  {
    "loss": 2.2889,
    "grad_norm": 2.128316879272461,
    "learning_rate": 2.079646017699115e-05,
    "epoch": 1.168141592920354,
    "step": 660
  },
  {
    "loss": 2.3159,
    "grad_norm": 1.7651296854019165,
    "learning_rate": 2.0353982300884957e-05,
    "epoch": 1.1858407079646018,
    "step": 670
  },
  {
    "loss": 2.2148,
    "grad_norm": 1.4271427392959595,
    "learning_rate": 1.991150442477876e-05,
    "epoch": 1.2035398230088497,
    "step": 680
  },
  {
    "loss": 2.1907,
    "grad_norm": 1.8040995597839355,
    "learning_rate": 1.946902654867257e-05,
    "epoch": 1.2212389380530975,
    "step": 690
  },
  {
    "loss": 2.2362,
    "grad_norm": 1.9243143796920776,
    "learning_rate": 1.9026548672566372e-05,
    "epoch": 1.238938053097345,
    "step": 700
  },
  {
    "loss": 2.1719,
    "grad_norm": 1.4556715488433838,
    "learning_rate": 1.858407079646018e-05,
    "epoch": 1.2566371681415929,
    "step": 710
  },
  {
    "loss": 2.3477,
    "grad_norm": 1.8083131313323975,
    "learning_rate": 1.8141592920353983e-05,
    "epoch": 1.2743362831858407,
    "step": 720
  },
  {
    "loss": 2.2888,
    "grad_norm": 2.1934759616851807,
    "learning_rate": 1.7699115044247787e-05,
    "epoch": 1.2920353982300885,
    "step": 730
  },
  {
    "loss": 2.2131,
    "grad_norm": 1.861983299255371,
    "learning_rate": 1.7256637168141594e-05,
    "epoch": 1.3097345132743363,
    "step": 740
  },
  {
    "loss": 2.2522,
    "grad_norm": 1.9416931867599487,
    "learning_rate": 1.6814159292035402e-05,
    "epoch": 1.3274336283185841,
    "step": 750
  },
  {
    "loss": 2.3345,
    "grad_norm": 1.6680678129196167,
    "learning_rate": 1.6371681415929206e-05,
    "epoch": 1.3451327433628317,
    "step": 760
  },
  {
    "loss": 2.243,
    "grad_norm": 1.794723629951477,
    "learning_rate": 1.592920353982301e-05,
    "epoch": 1.3628318584070795,
    "step": 770
  },
  {
    "loss": 2.2893,
    "grad_norm": 1.8596407175064087,
    "learning_rate": 1.5486725663716813e-05,
    "epoch": 1.3805309734513274,
    "step": 780
  },
  {
    "loss": 2.2288,
    "grad_norm": 1.944309115409851,
    "learning_rate": 1.504424778761062e-05,
    "epoch": 1.3982300884955752,
    "step": 790
  },
  {
    "loss": 2.3052,
    "grad_norm": 1.9052109718322754,
    "learning_rate": 1.4601769911504426e-05,
    "epoch": 1.415929203539823,
    "step": 800
  },
  {
    "loss": 2.0789,
    "grad_norm": 1.813560128211975,
    "learning_rate": 1.415929203539823e-05,
    "epoch": 1.4336283185840708,
    "step": 810
  },
  {
    "loss": 2.2793,
    "grad_norm": 1.988527536392212,
    "learning_rate": 1.3716814159292036e-05,
    "epoch": 1.4513274336283186,
    "step": 820
  },
  {
    "loss": 2.327,
    "grad_norm": 2.110689640045166,
    "learning_rate": 1.3274336283185843e-05,
    "epoch": 1.4690265486725664,
    "step": 830
  },
  {
    "loss": 2.282,
    "grad_norm": 1.812869668006897,
    "learning_rate": 1.2831858407079647e-05,
    "epoch": 1.4867256637168142,
    "step": 840
  },
  {
    "loss": 2.1307,
    "grad_norm": 1.9135193824768066,
    "learning_rate": 1.2389380530973452e-05,
    "epoch": 1.504424778761062,
    "step": 850
  },
  {
    "loss": 2.2834,
    "grad_norm": 1.9862174987792969,
    "learning_rate": 1.1946902654867258e-05,
    "epoch": 1.5221238938053099,
    "step": 860
  },
  {
    "loss": 2.2565,
    "grad_norm": 1.858086347579956,
    "learning_rate": 1.1504424778761062e-05,
    "epoch": 1.5398230088495575,
    "step": 870
  },
  {
    "loss": 2.1947,
    "grad_norm": 1.7941893339157104,
    "learning_rate": 1.1061946902654869e-05,
    "epoch": 1.5575221238938053,
    "step": 880
  },
  {
    "loss": 2.2339,
    "grad_norm": 1.9410710334777832,
    "learning_rate": 1.0619469026548673e-05,
    "epoch": 1.575221238938053,
    "step": 890
  },
  {
    "loss": 2.2354,
    "grad_norm": 1.7172718048095703,
    "learning_rate": 1.0176991150442479e-05,
    "epoch": 1.592920353982301,
    "step": 900
  },
  {
    "loss": 2.3121,
    "grad_norm": 1.7255445718765259,
    "learning_rate": 9.734513274336284e-06,
    "epoch": 1.6106194690265485,
    "step": 910
  },
  {
    "loss": 2.2377,
    "grad_norm": 1.8904085159301758,
    "learning_rate": 9.29203539823009e-06,
    "epoch": 1.6283185840707963,
    "step": 920
  },
  {
    "loss": 2.2136,
    "grad_norm": 1.7782626152038574,
    "learning_rate": 8.849557522123894e-06,
    "epoch": 1.6460176991150441,
    "step": 930
  },
  {
    "loss": 2.1927,
    "grad_norm": 1.822356939315796,
    "learning_rate": 8.407079646017701e-06,
    "epoch": 1.663716814159292,
    "step": 940
  },
  {
    "loss": 2.0358,
    "grad_norm": 1.5199676752090454,
    "learning_rate": 7.964601769911505e-06,
    "epoch": 1.6814159292035398,
    "step": 950
  },
  {
    "loss": 2.2164,
    "grad_norm": 1.919173002243042,
    "learning_rate": 7.52212389380531e-06,
    "epoch": 1.6991150442477876,
    "step": 960
  },
  {
    "loss": 2.3253,
    "grad_norm": 1.8059217929840088,
    "learning_rate": 7.079646017699115e-06,
    "epoch": 1.7168141592920354,
    "step": 970
  },
  {
    "loss": 2.1827,
    "grad_norm": 2.0878570079803467,
    "learning_rate": 6.6371681415929215e-06,
    "epoch": 1.7345132743362832,
    "step": 980
  },
  {
    "loss": 2.2206,
    "grad_norm": 1.991294026374817,
    "learning_rate": 6.194690265486726e-06,
    "epoch": 1.752212389380531,
    "step": 990
  },
  {
    "loss": 2.2796,
    "grad_norm": 1.9287645816802979,
    "learning_rate": 5.752212389380531e-06,
    "epoch": 1.7699115044247788,
    "step": 1000
  },
  {
    "loss": 2.2156,
    "grad_norm": 1.8298494815826416,
    "learning_rate": 5.3097345132743365e-06,
    "epoch": 1.7876106194690267,
    "step": 1010
  },
  {
    "loss": 2.1491,
    "grad_norm": 1.6964576244354248,
    "learning_rate": 4.867256637168142e-06,
    "epoch": 1.8053097345132745,
    "step": 1020
  },
  {
    "loss": 2.1433,
    "grad_norm": 1.8080228567123413,
    "learning_rate": 4.424778761061947e-06,
    "epoch": 1.823008849557522,
    "step": 1030
  },
  {
    "loss": 2.3339,
    "grad_norm": 1.939254641532898,
    "learning_rate": 3.982300884955752e-06,
    "epoch": 1.8407079646017699,
    "step": 1040
  },
  {
    "loss": 2.252,
    "grad_norm": 1.8926523923873901,
    "learning_rate": 3.5398230088495575e-06,
    "epoch": 1.8584070796460177,
    "step": 1050
  },
  {
    "loss": 2.2841,
    "grad_norm": 1.6270568370819092,
    "learning_rate": 3.097345132743363e-06,
    "epoch": 1.8761061946902655,
    "step": 1060
  },
  {
    "loss": 2.2615,
    "grad_norm": 1.7319533824920654,
    "learning_rate": 2.6548672566371683e-06,
    "epoch": 1.893805309734513,
    "step": 1070
  },
  {
    "loss": 2.2696,
    "grad_norm": 1.8606464862823486,
    "learning_rate": 2.2123893805309734e-06,
    "epoch": 1.911504424778761,
    "step": 1080
  },
  {
    "loss": 2.2365,
    "grad_norm": 1.845365285873413,
    "learning_rate": 1.7699115044247788e-06,
    "epoch": 1.9292035398230087,
    "step": 1090
  },
  {
    "loss": 2.1365,
    "grad_norm": 1.973345160484314,
    "learning_rate": 1.3274336283185841e-06,
    "epoch": 1.9469026548672566,
    "step": 1100
  },
  {
    "loss": 2.1812,
    "grad_norm": 1.8496919870376587,
    "learning_rate": 8.849557522123894e-07,
    "epoch": 1.9646017699115044,
    "step": 1110
  },
  {
    "loss": 2.1692,
    "grad_norm": 2.0190272331237793,
    "learning_rate": 4.424778761061947e-07,
    "epoch": 1.9823008849557522,
    "step": 1120
  },
  {
    "loss": 2.297,
    "grad_norm": 2.412532329559326,
    "learning_rate": 0.0,
    "epoch": 2.0,
    "step": 1130
  },
  {
    "eval_loss": 2.3059744834899902,
    "eval_runtime": 36.3449,
    "eval_samples_per_second": 13.069,
    "eval_steps_per_second": 0.825,
    "epoch": 2.0,
    "step": 1130
  },
  {
    "train_runtime": 7099.6897,
    "train_samples_per_second": 2.545,
    "train_steps_per_second": 0.159,
    "total_flos": 4722069602304000.0,
    "train_loss": 2.485286334552596,
    "epoch": 2.0,
    "step": 1130
  }
]