import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
MODEL_PATH = "app/cartman_model_best"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(f'{MODEL_PATH}\checkpoint-565')
model.to('cpu')
model.eval()

def generate_cartman_reply(context: str, max_new_tokens: int = 64) -> str:
    if not context.strip().endswith("[CARTMAN]"):
        context = context.strip() + "\n[CARTMAN]"

    inputs = tokenizer(context, return_tensors="pt", truncation=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(outputs[0])
    print("üß† Generated raw text:\n", decoded)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Ä–µ–ø–ª–∏–∫—É [CARTMAN] ... [/CARTMAN]
    start = decoded.find("[CARTMAN]")
    end = decoded.find("[/CARTMAN]", start)

    if start != -1 and end != -1:
        return decoded[start + len("[CARTMAN]"):end].strip()

    # fallback: –≤–∑—è—Ç—å –≤—Å—ë –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ [CARTMAN]
    fallback = decoded.split("[CARTMAN]")[-1]
    fallback = fallback.split("[OTHER]")[0]
    return fallback.strip()


# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞
if __name__ == "__main__":
    test_context = """
    [OTHER] –≠–π, –ö–∞—Ä—Ç–º–∞–Ω, —Ç—ã –æ–ø—è—Ç—å –ø—Ä–æ—Å–ø–∞–ª —à–∫–æ–ª—É? [/OTHER]
    [OTHER] –¢—ã –≤—á–µ—Ä–∞ –æ–±–µ—â–∞–ª –ø—Ä–∏–π—Ç–∏ –ø–æ—Ä–∞–Ω—å—à–µ. [/OTHER]
    [CARTMAN]"""

    reply = generate_cartman_reply(test_context)
    print(f"[CARTMAN] {reply}")
